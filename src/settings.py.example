"""
Shared settings
"""

# The options for Redis
REDIS_OPTS = {
  'host': 'localhost',
  'port': 6379,
}

# If you have a Graphite host set up, set this metric to get graphs on
# Skyline and Horizon. Don't include http:// since this is used for carbon host as well.
GRAPHITE_HOST = 'your_graphite_host.com'

# The Graph url used to link to Graphite (Or another graphite dashboard)
# %s will be replaced by the metric name
GRAPH_URL = 'http://' + GRAPHITE_HOST + '/render/?width=1400&from=-1hour&target=%s'

# If you have a Graphite host set up, set its Carbon port.
CARBON_PORT = 2003

"""
Analyzer settings
"""
# This is the duration, in seconds, for a metric to become 'stale' and for
# the analyzer to ignore it until new datapoints are added. 'Staleness' means
# that a datapoint has not been added for STALE_PERIOD seconds.
STALE_PERIOD = 500

# This is the minimum length of a timeseries, in datapoints, for the analyzer
# to recognize it as a complete series.
MIN_TOLERABLE_LENGTH = 1

# Sometimes a metric will continually transmit the same number. There's no need
# to analyze metrics that remain boring like this, so this setting determines
# the amount of boring datapoints that will be allowed to accumulate before the
# analyzer skips over the metric. If the metric becomes noisy again, the
# analyzer will stop ignoring it.
MAX_TOLERABLE_BOREDOM = 100

# By default, the analyzer skips a metric if it it has transmitted a single
# number MAX_TOLERABLE_BOREDOM times. Change this setting if you wish the size
# of the ignored set to be higher (ie, ignore the metric if there have only
# been two different values for the past MAX_TOLERABLE_BOREDOM datapoints).
# This is useful for timeseries that often oscillate between two values.
BOREDOM_SET_SIZE = 1

# These are the algorithms that the Analyzer will run. To add a new algorithm,
# you must both define the algorithm in algorithms.py and add its name here.
ALGORITHMS = [
    'first_hour_average',
    'mean_subtraction_cumulation',
    'stddev_from_average',
    'stddev_from_moving_average',
    'least_squares',
    'grubbs',
    'histogram_bins',
    'median_absolute_deviation',
    'ks_test',
]

# This is the config for which metrics to alert on and which strategy to use for each.
# Alerts will not fire twice within EXPIRATION_TIME, even if they trigger again.
# Schema: (
#          ("metric1", "smtp", EXPIRATION_TIME),
#          ("metric2", "pagerduty", EXPIRATION_TIME),
#          ("metric3", "hipchat", EXPIRATION_TIME),
#          ("stats", "syslog", EXPIRATION_TIME),
# Wildcard namespaces can be used as well
#          ("metric4.thing.*.requests", "stmp", EXPIRATION_TIME),
#         )
ALERTS = (
    ("skyline", "smtp", 1800),
)

# Each alert module requires additional information.
SMTP_OPTS = {
    # This specifies the mailserver to connect to. If user or password are blank,
    # no authentication is used.
    "host": "127.0.0.1:25",
    "user": "skyline",
    "password": "",

    # This specifies the sender of email alerts.
    "sender": "skyline-alerts@etsy.com",
    # recipients is a dictionary mapping metric names
    # (exactly matching those listed in ALERTS) to an array of e-mail addresses
    "recipients": {
        "skyline": ["abe@etsy.com", "you@yourcompany.com"],
    },
}

# HipChat alerts require python-simple-hipchat
HIPCHAT_OPTS = {
    "auth_token": "pagerduty_auth_token",
    # list of hipchat room_ids to notify about each anomaly
    # (similar to SMTP_OPTS['recipients'])
    "rooms": {
        "skyline": (12345,),
    },
    # Background color of hipchat messages
    # (One of "yellow", "red", "green", "purple", "gray", or "random".)
    "color": "purple",
}

# PagerDuty alerts require pygerduty
PAGERDUTY_OPTS = {
    # Your pagerduty subdomain and auth token
    "subdomain": "example",
    "auth_token": "your_pagerduty_auth_token",
    # Service API key (shown on the detail page of a "Generic API" service)
    "key": "your_pagerduty_service_api_key",
}

# syslog alerts requires an ident
# Adds a LOG_WARNING message to the LOG_LOCAL4 which is local and will ship to
# any syslog or rsyslog down the line. The EXPIRATION_TIME for the syslog alert
# method should be set to 1 to fire every anomaly into the syslog
SYSLOG_OPTS = {
    "ident": "skyline",
}


"""
Horizon settings
"""
# This is the number of Roomba processes that will be spawned to trim
# timeseries in order to keep them at FULL_DURATION. Keep this number small,
# as it is not important that metrics be exactly FULL_DURATION *all* the time.
ROOMBA_PROCESSES = 1

# Normally Roomba will clean up everything that is older than FULL_DURATION
# if you have metrics that are not coming in every second, it can happen
# that you'll end up with INCOMPLETE metrics.
# With this setting Roomba will clean up evertyhing that is older than
# FULL_DURATION + ROOMBA_GRACE_TIME
ROOMBA_GRACE_TIME = 600

# The Horizon agent will ignore incoming datapoints if their timestamp
# is older than MAX_RESOLUTION seconds ago.
MAX_RESOLUTION = 1000

# These are metrics that, for whatever reason, you do not want to store in Skyline. Each entry in the blacklist and
# whitelist is a regex, which behaves much like the graphite/carbon RegexList. The Listener will check to see if each
# incoming metric is not in the blacklist and in the whitelist. If a list is empty, all entries will pass through it.
BLACKLIST = [
    '^skyline\.'
    '^example\.statsd\.metric$',
    '^another\.example\..*',
    # if you use statsd, these can result in many near-equal series
    #'_90$',
    #'\.lower$',
    #'\.upper$',
    #'\.median$',
    #'\.count_ps$',
    #'\.sum$',
]

WHITELIST = []
